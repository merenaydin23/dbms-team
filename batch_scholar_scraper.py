import time
import random
import glob
import os
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
import pandas as pd
from scholar_scraper_v3 import (
    get_author_profile_url,
    get_all_article_links_from_profile,
    scrape_article_metadata
)


# ------------------------------------------------------------------
# ------------------ Yazar Listesi --------------------------------
# ------------------------------------------------------------------
# Buraya veri Ã§ekmek istediÄŸiniz yazar isimlerini ekleyin
AUTHOR_LIST = [
    "Ä°BRAHÄ°M TÃœRKOÄLU",
    "ENGÄ°N AVCI",
    "RESUL DAÅ",
    "ERKAN TANYILDIZI",
    "MURAT KARABATAK",
    "FATÄ°H Ã–ZKAYNAK",
    "Ã–ZAL YILDIRIM",
    "MUHAMMET BAYKARA",
    "YAMAN AKBULUT",
    "BÄ°HTER DAÅ",
    "FERHAT UÃ‡AR",
    "MURAT AYDOÄAN",
    "VAHTETTÄ°N CEM BAYDOÄAN",
    "ALEV KAYA",
    "OÄUZHAN KATAR",
    "ZÃœLFÄ°YE BEYZA METÄ°N",
    "Ã–MER MÄ°RAÃ‡ KÃ–KÃ‡AM",
    "BÄ°LAL ALATAÅ",
    "FATÄ°H Ã–ZYURT",
    "MUHAMMED TALO",
    "SEDA ARSLAN TUNCER",
    "FATÄ°H DEMÄ°R",
    "Ã–ZGÃœR KARADUMAN",
    "FEYZA ALTUNBEY Ã–ZBAY",
    "SÄ°NEM AKYOL",
    "ESRA GÃœNDOÄAN",
    "Ä°RFAN KILIÃ‡",
    "KÃœBRA ARSLANOÄLU",
    "NÄ°GAR Ã–ZBEY",
    "ESRA YÃœZGEÃ‡ Ã–ZDEMÄ°R",
]


# ------------------------------------------------------------------
# ------------------ YardÄ±mcÄ± Fonksiyonlar -------------------------
# ------------------------------------------------------------------
PARQUET_DIR = "parquet_data"  # Parquet dosyalarÄ±nÄ±n kaydedileceÄŸi klasÃ¶r


def ensure_parquet_dir():
    """Parquet klasÃ¶rÃ¼nÃ¼n var olduÄŸundan emin olur."""
    if not os.path.exists(PARQUET_DIR):
        os.makedirs(PARQUET_DIR)


def get_parquet_filename(author_name):
    """Yazar adÄ±ndan parquet dosya adÄ±nÄ± oluÅŸturur (scholar_scraper_v3.py ile aynÄ± mantÄ±k)."""
    safe_name = "".join([c if c.isalnum() else "_" for c in author_name])
    return os.path.join(PARQUET_DIR, f"{safe_name}_metadata.parquet")


def parquet_exists(author_name):
    """Yazar iÃ§in parquet dosyasÄ±nÄ±n mevcut olup olmadÄ±ÄŸÄ±nÄ± kontrol eder."""
    filename = get_parquet_filename(author_name)
    return os.path.exists(filename)


def save_to_parquet(data, filename_prefix):
    """Verileri parquet formatÄ±nda kaydeder (klasÃ¶r iÃ§ine)."""
    ensure_parquet_dir()
    safe_name = "".join([c if c.isalnum() else "_" for c in filename_prefix])
    filename = os.path.join(PARQUET_DIR, f"{safe_name}_metadata.parquet")
    
    df = pd.DataFrame(data)
    
    df.to_parquet(
        filename,
        engine="pyarrow",
        compression="snappy",
        index=False
    )
    
    print(f"\nVeriler Parquet formatÄ±nda kaydedildi: {filename}")


# ------------------------------------------------------------------
# ------------------ Batch Scraping Fonksiyonu ---------------------
# ------------------------------------------------------------------
def scrape_author(driver, author_name):
    """Tek bir yazar iÃ§in veri Ã§eker ve parquet dosyasÄ± oluÅŸturur."""
    print(f"\n{'='*60}")
    print(f"'{author_name}' iÃ§in iÅŸlem baÅŸlÄ±yor...")
    print(f"{'='*60}")
    
    try:
        profile_url = get_author_profile_url(driver, author_name)
        
        if not profile_url:
            print(f"âš ï¸  '{author_name}' iÃ§in profil bulunamadÄ±. AtlanÄ±yor...")
            return None
        
        print(f"âœ“ Profil bulundu: {profile_url}")
        print("Makale listesi geniÅŸletiliyor...")
        article_links = get_all_article_links_from_profile(driver, profile_url)
        print(f"Toplam {len(article_links)} makale linki bulundu. Veri Ã§ekme baÅŸlÄ±yor...")
        
        if not article_links:
            print(f"âš ï¸  '{author_name}' iÃ§in makale bulunamadÄ±.")
            return None
        
        results = []
        for i, link in enumerate(article_links, 1):
            print(f"[{i}/{len(article_links)}] Ä°ÅŸleniyor...")
            
            data = scrape_article_metadata(driver, link)
            
            if data:
                data['profile_owner'] = author_name
                results.append(data)
                
                print(f"   -> BaÅŸlÄ±k: {data['title'][:50]}...")
                print(f"   -> DOI: {data['doi']}")
            
            # Google Scholar'Ä±n robot kontrolÃ¼ne takÄ±lmamak iÃ§in rastgele bekleme
            time.sleep(random.uniform(2, 4))
        
        if results:
            save_to_parquet(results, author_name)
            print(f"âœ“ '{author_name}' iÃ§in {len(results)} makale verisi kaydedildi.")
            return author_name
        else:
            print(f"âš ï¸  '{author_name}' iÃ§in hiÃ§bir makale verisi Ã§ekilemedi.")
            return None
            
    except Exception as e:
        print(f"âŒ '{author_name}' iÃ§in hata oluÅŸtu: {e}")
        return None


def convert_parquets_to_csv(output_filename="all_authors_combined.csv"):
    """TÃ¼m parquet dosyalarÄ±nÄ± birleÅŸtirip CSV'ye Ã§evirir."""
    print(f"\n{'='*60}")
    print("Parquet dosyalarÄ± CSV'ye Ã§evriliyor...")
    print(f"{'='*60}")
    
    # KlasÃ¶rdeki tÃ¼m parquet dosyalarÄ±nÄ± bul
    parquet_pattern = os.path.join(PARQUET_DIR, "*_metadata.parquet")
    parquet_files = glob.glob(parquet_pattern)
    
    if not parquet_files:
        print(f"âš ï¸  '{PARQUET_DIR}' klasÃ¶rÃ¼nde hiÃ§ parquet dosyasÄ± bulunamadÄ±.")
        return
    
    print(f"Bulunan parquet dosyalarÄ±: {len(parquet_files)}")
    
    # TÃ¼m parquet dosyalarÄ±nÄ± oku ve birleÅŸtir
    all_dataframes = []
    for parquet_file in parquet_files:
        try:
            df = pd.read_parquet(parquet_file)
            all_dataframes.append(df)
            file_basename = os.path.basename(parquet_file)
            print(f"âœ“ {file_basename} okundu ({len(df)} satÄ±r)")
        except Exception as e:
            file_basename = os.path.basename(parquet_file)
            print(f"âš ï¸  {file_basename} okunurken hata: {e}")
    
    if not all_dataframes:
        print("âš ï¸  HiÃ§bir parquet dosyasÄ± okunamadÄ±.")
        return
    
    # TÃ¼m dataframe'leri birleÅŸtir
    combined_df = pd.concat(all_dataframes, ignore_index=True)
    
    # CSV'ye kaydet
    combined_df.to_csv(output_filename, index=False, encoding='utf-8-sig')
    
    print(f"\nâœ“ TÃ¼m veriler birleÅŸtirildi ve CSV'ye kaydedildi: {output_filename}")
    print(f"  Toplam satÄ±r sayÄ±sÄ±: {len(combined_df)}")
    print(f"  Toplam sÃ¼tun sayÄ±sÄ±: {len(combined_df.columns)}")
    print(f"  SÃ¼tunlar: {', '.join(combined_df.columns.tolist())}")


# ------------------------------------------------------------------
# -------------------------------- MAIN ----------------------------
# ------------------------------------------------------------------
def main():
    if not AUTHOR_LIST:
        print("âš ï¸  AUTHOR_LIST boÅŸ! LÃ¼tfen yazar isimlerini ekleyin.")
        print("   DosyayÄ± aÃ§Ä±p AUTHOR_LIST dizisine yazar isimlerini ekleyin.")
        return
    
    # Parquet klasÃ¶rÃ¼nÃ¼n var olduÄŸundan emin ol
    ensure_parquet_dir()
    
    print(f"Toplam {len(AUTHOR_LIST)} yazar iÃ§in veri Ã§ekme iÅŸlemi baÅŸlÄ±yor...")
    
    # Mevcut parquet dosyalarÄ±nÄ± kontrol et
    existing_authors = [author for author in AUTHOR_LIST if parquet_exists(author)]
    new_authors = [author for author in AUTHOR_LIST if not parquet_exists(author)]
    
    if existing_authors:
        print(f"\nâ­ï¸  {len(existing_authors)} yazar iÃ§in parquet dosyasÄ± zaten mevcut (atlanacak):")
        for author in existing_authors:
            print(f"   - {author} ({get_parquet_filename(author)})")
    
    if new_authors:
        print(f"\nğŸ”„ {len(new_authors)} yazar iÃ§in yeni veri Ã§ekilecek:")
        for author in new_authors:
            print(f"   - {author}")
    
    # EÄŸer hiÃ§ yeni yazar yoksa, sadece CSV birleÅŸtirme yap
    if not new_authors:
        print("\nâš ï¸  TÃ¼m yazarlar iÃ§in parquet dosyasÄ± zaten mevcut. Sadece CSV birleÅŸtirme yapÄ±lacak.")
        convert_parquets_to_csv()
        return
    
    # TarayÄ±cÄ± AyarlarÄ±
    options = Options()
    # options.add_argument('--headless')  # Arka planda Ã§alÄ±ÅŸmasÄ±nÄ± isterseniz yorum satÄ±rÄ±nÄ± kaldÄ±rÄ±n
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-gpu')
    options.add_argument(
        "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36")
    
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=options)
    
    successful_authors = []
    failed_authors = []
    skipped_authors = existing_authors.copy()  # BaÅŸtan mevcut olanlarÄ± ekle
    
    try:
        for idx, author_name in enumerate(AUTHOR_LIST, 1):
            print(f"\n[{idx}/{len(AUTHOR_LIST)}] Yazar: {author_name}")
            
            # EÄŸer parquet dosyasÄ± zaten varsa, atla
            if parquet_exists(author_name):
                filename = get_parquet_filename(author_name)
                print(f"â­ï¸  '{author_name}' iÃ§in parquet dosyasÄ± zaten mevcut: {filename}")
                print(f"   Bu yazar atlanÄ±yor, mevcut dosya kullanÄ±lacak.")
                if author_name not in skipped_authors:
                    skipped_authors.append(author_name)
                continue
            
            result = scrape_author(driver, author_name)
            
            if result:
                successful_authors.append(author_name)
            else:
                failed_authors.append(author_name)
            
            # Yazar arasÄ± bekleme (robot kontrolÃ¼ iÃ§in)
            if idx < len(AUTHOR_LIST):
                wait_time = random.uniform(3, 6)
                print(f"\nâ³ Sonraki yazara geÃ§meden Ã¶nce {wait_time:.1f} saniye bekleniyor...")
                time.sleep(wait_time)
        
        # Ã–zet
        print(f"\n{'='*60}")
        print("Ä°ÅLEM Ã–ZETÄ°")
        print(f"{'='*60}")
        print(f"âœ“ Yeni Ã§ekilen: {len(successful_authors)} yazar")
        if successful_authors:
            print(f"  -> {', '.join(successful_authors)}")
        print(f"â­ï¸  Atlanan (zaten mevcut): {len(skipped_authors)} yazar")
        if skipped_authors:
            print(f"  -> {', '.join(skipped_authors)}")
        print(f"âŒ BaÅŸarÄ±sÄ±z: {len(failed_authors)} yazar")
        if failed_authors:
            print(f"  -> {', '.join(failed_authors)}")
        
        # Parquet dosyalarÄ±nÄ± CSV'ye Ã§evir (yeni Ã§ekilenler + mevcut olanlar)
        total_parquet_count = len(successful_authors) + len(skipped_authors)
        if total_parquet_count > 0:
            convert_parquets_to_csv()
        else:
            print("\nâš ï¸  HiÃ§bir yazar iÃ§in parquet dosyasÄ± bulunamadÄ±ÄŸÄ± iÃ§in CSV oluÅŸturulamadÄ±.")
            
    except Exception as e:
        print(f"\nâŒ Beklenmedik bir hata oluÅŸtu: {e}")
    finally:
        driver.quit()
        print("\nâœ“ TarayÄ±cÄ± kapatÄ±ldÄ±.")


if __name__ == '__main__':
    main()

